import os
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings

# Configuration
PDF_PATH = os.path.join(os.path.dirname(__file__), "../data/UET_Prospectus.pdf")
DB_PATH = os.path.join(os.path.dirname(__file__), "../data/vector_db")

def ingest_data():
    print("ğŸš€ Starting Data Ingestion...")
    
    # 1. Load PDF
    if not os.path.exists(PDF_PATH):
        raise FileNotFoundError(f"PDF not found at {PDF_PATH}. Please add the file.")
    
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()
    print(f"ğŸ“„ Loaded {len(docs)} pages.")

    # 2. Split Text
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    splits = text_splitter.split_documents(docs)
    print(f"âœ‚ï¸ Split into {len(splits)} chunks.")

    # 3. Create Vector Store
    # --- FIX: Switch to a dedicated embedding model ---
    print("â³ Generating Embeddings (this may take a moment)...")
    embeddings = OllamaEmbeddings(model="nomic-embed-text") 
    
    if os.path.exists(DB_PATH):
        print("ğŸ—‘ï¸ Clearing old database...")
        import shutil
        shutil.rmtree(DB_PATH)

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=DB_PATH
    )
    print(f"ğŸ’¾ Vector Database saved to {DB_PATH}")

if __name__ == "__main__":
    ingest_data()